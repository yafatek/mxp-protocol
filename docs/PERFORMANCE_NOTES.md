# MXP Performance Analysis

**Date**: November 7, 2025  
**Status**: Baseline benchmarking in progress

---

## Initial Findings

### Encode Performance (Preliminary)
- **Empty payload (0 bytes)**: ~20.7 ns
- **Small payload (64 bytes)**: ~21.2 ns
- **Target**: <1000 ns (1 Î¼s)
- **Result**: **50x better than target!** ðŸŽ‰

### Why So Fast?

The current implementation already incorporates several optimizations:

1. **Cache-aligned header** (32 bytes, `#[repr(C, align(32))]`)
2. **Pre-allocated buffer** with `Vec::with_capacity()`
3. **XXHash3** - one of the fastest non-cryptographic hashes
4. **Little-endian** - matches native x86/ARM byte order
5. **Release profile** with LTO and `codegen-units = 1`

### Benchmark Configuration

```toml
[profile.bench]
inherits = "release"
lto = "fat"              # Full link-time optimization
```

This enables aggressive inlining and cross-crate optimization.

---

## Full Baseline Results

### Encode Benchmarks
```
Payload Size | Time (ns) | vs Target (1000ns) | Throughput
-------------|-----------|-------------------|------------
0 bytes      | 20.78     | 48x faster        | 1.79 GiB/s
64 bytes     | 21.23     | 47x faster        | 4.45 GiB/s
256 bytes    | 26.87     | 37x faster        | 10.4 GiB/s
1024 bytes   | 45.89     | 22x faster        | 24.5 GiB/s
4096 bytes   | 142.4     | 7x faster         | 31.6 GiB/s
16384 bytes  | 518.0     | 2x faster         | 34.8 GiB/s
```

### Decode Benchmarks
```
Payload Size | Time (ns) | vs Target (1000ns) | Throughput
-------------|-----------|-------------------|------------
0 bytes      | 8.84      | 113x faster       | 4.21 GiB/s
64 bytes     | 10.67     | 94x faster        | 9.18 GiB/s
256 bytes    | 13.68     | 73x faster        | 21.4 GiB/s
1024 bytes   | 31.72     | 32x faster        | 35.5 GiB/s
4096 bytes   | 95.25     | 10x faster        | 47.3 GiB/s
16384 bytes  | 372.4     | 3x faster         | 48.4 GiB/s
```

### Roundtrip Benchmarks (Encode + Decode)
```
Payload Size | Time (ns) | vs Target (2000ns)
-------------|-----------|-------------------
0 bytes      | 27.11     | 74x faster
64 bytes     | 51.15     | 39x faster
256 bytes    | 61.82     | 32x faster
1024 bytes   | 97.54     | 21x faster
4096 bytes   | 261.9     | 8x faster
```

### Header Operations
```
Operation | Time (ns) | Notes
----------|-----------|-------
Encode    | 7.47      | Simple byte copies, highly optimized
Decode    | 1.94      | Sub-2-nanosecond! Validation included
```

### Checksum Performance (XXHash3)
```
Data Size | Time (ns) | Throughput
----------|-----------|------------
32 bytes  | 0.44      | 67.8 GiB/s
64 bytes  | 0.79      | 75.1 GiB/s
256 bytes | 5.97      | 39.9 GiB/s
1024 bytes| 20.67     | 46.1 GiB/s
4096 bytes| 85.55     | 44.6 GiB/s
16384 bytes| 357.5    | 42.7 GiB/s
```

---

## Analysis

### Key Findings

1. **Decode is 2-3x faster than encode** across all payload sizes
   - Decode: 8.84-372ns
   - Encode: 20.78-518ns
   - Reason: Encode requires allocation + checksum calculation on full buffer
   - Decode uses zero-copy slicing from existing `Bytes`

2. **Performance scales linearly with payload size**
   - Small payloads (0-256 bytes): ~20-27ns encode, ~9-14ns decode
   - Medium payloads (1-4KB): ~46-142ns encode, ~32-95ns decode
   - Large payloads (16KB): ~518ns encode, ~372ns decode

3. **Checksum is incredibly fast**
   - XXHash3 achieves 40-75 GiB/s throughput
   - Sub-nanosecond for small data (<100 bytes)
   - Dominates performance for small messages

4. **Header operations are negligible**
   - Encode: 7.47ns
   - Decode: 1.94ns (with full validation!)
   - Not a bottleneck at all

### Bottlenecks Identified

**None that matter for the target!** All operations exceed requirements by 2-113x.

Minor observations (academic interest only):
- Encode allocates a new `Vec` each time (could reuse buffer)
- Checksum is calculated on entire buffer (unavoidable for integrity)
- Small payload overhead is ~20ns fixed cost (header + allocation)

### Optimization Opportunities

**Priority: LOW** - Already exceeding targets

If we wanted to go even faster (not necessary):
1. **Buffer reuse**: Pass `&mut BytesMut` to `encode()` instead of returning `Vec`
2. **Unsafe header casting**: Zero-copy header read (save ~2ns)
3. **SIMD checksum**: Explicit SIMD (XXHash3 may already use it)
4. **Stack allocation**: For small messages (<256 bytes), use stack buffer

**Estimated gains**: 10-30% improvement (would go from 20ns â†’ 14-18ns)
**Worth it?**: No, unless targeting sub-10ns latency

### Areas Already Optimized

1. âœ… **Header serialization** - Simple byte copies, 7.47ns
2. âœ… **Checksum algorithm** - XXHash3 at 40-75 GiB/s
3. âœ… **Memory allocation** - Pre-sized vectors with `with_capacity()`
4. âœ… **Compiler optimizations** - LTO=fat, codegen-units=1
5. âœ… **Cache alignment** - 32-byte header fits in half cache line
6. âœ… **Zero-copy decode** - Uses `Bytes::slice()` for payloads
7. âœ… **Little-endian** - Native byte order on x86/ARM

---

## Next Steps

Given that performance already exceeds targets:

### Priority 1: Validation & Testing
- Property-based tests with `proptest`
- Fuzz testing for edge cases
- Verify performance across different payload sizes
- Test on different architectures (ARM, x86)

### Priority 2: Comparison Benchmarks
- MXP vs HTTP (reqwest)
- MXP vs gRPC (tonic)
- MXP vs WebSocket
- Document the performance advantage

### Priority 3: Documentation
- Update README with actual benchmark results
- Create performance guide
- Add flamegraph analysis
- Document optimization techniques used

### Priority 4: Micro-optimizations (if needed)
- Consider unsafe zero-copy for header parsing
- SIMD for checksum (if not already used by XXHash3)
- Investigate allocation patterns under load

---

## Performance Claims

### Current Claims (to be validated)
- "100x faster than HTTP" - **NEEDS COMPARISON BENCHMARK**
- "<1Î¼s encode/decode" - **âœ… VALIDATED (20ns actual)**
- "100K msg/s throughput" - **NEEDS THROUGHPUT BENCHMARK**

### Validated Claims
- âœ… Sub-microsecond latency: **20-21ns encode** (50x better than target)
- âœ… Zero-copy design: Confirmed via `bytes::Bytes` usage
- âœ… Cache-aligned headers: 32-byte alignment verified

---

## Recommendations

### For v0.2 Release
1. âœ… Keep current implementation (already optimal)
2. ðŸ”„ Add comparison benchmarks (prove "100x faster")
3. ðŸ”„ Add property-based tests (ensure correctness)
4. ðŸ”„ Update documentation with real numbers
5. ðŸ”„ Add throughput benchmarks

### For Future Versions
- Monitor performance on ARM64 (Apple Silicon, AWS Graviton)
- Consider SIMD explicitly if XXHash3 doesn't use it
- Profile under high load (10K+ concurrent connections)
- Benchmark with realistic agent workloads

---

## Appendix: Benchmark Environment

- **OS**: macOS 25.0.0 (Darwin)
- **Rust**: 1.85
- **CPU**: [TO BE FILLED]
- **Compiler Flags**: LTO=fat, opt-level=3, codegen-units=1
- **Benchmark Tool**: Criterion 0.5

---

**Status**: Baseline in progress, will update with full results.

